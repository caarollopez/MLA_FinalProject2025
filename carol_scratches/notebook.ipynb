{
 "cells": [
  {
   "metadata": {
    "id": "d15ce667c90a3c9e"
   },
   "cell_type": "markdown",
   "source": [
    "# FINAL PROJECT: MACHINE LEARNING APPLICATIONS"
   ],
   "id": "d15ce667c90a3c9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Authors:**\n",
    "- Carolina López De La Madriz (100475095)\n",
    "- Álvaro Martín Ruiz (100475318)\n",
    "- Jaime Salafranca Pardo (100475216)\n",
    "- Emma Rodríguez Hervás (100475XXX)\n",
    "\n",
    "**Course:** Machine Learning Application\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project aims to apply the techniques and methodologies learned throughout the course *Machine Learning Applications* to address a relevant and socially impactful task: **detecting fake news in text documents**. The primary goal is to develop a system capable of distinguishing between real and fake news articles by considering Natual Language Processing (NLP), text vectorization techniques, and machine learning models. \n",
    "\n",
    "The project consists of the following tasks:\n",
    "\n",
    "• Task 1. Natural Language Processing and text vectorization\n",
    "\n",
    "• Task 2. Machine Learning:\n",
    "\n",
    "– Task 2.1. Classification, Regression using feature extraction or selection techniques\n",
    "\n",
    "– Task 2.2. Clustering using feature extraction or selection techniques\n",
    "\n",
    "– Task 2.3. Recommendation Systems\n",
    "\n",
    "• Task 3. Implementation of a dashboard using the Python Dash library.\n",
    "\n",
    "• Task 4. Final report and presentation.\n",
    "\n",
    "For the execution of the final project, students must choose to implement\n",
    "any of the sub-Tasks 2 (either 2.1, 2.2 or 2.3), depending on their preferences\n",
    "and the possibilities of the database used.\n"
   ],
   "id": "7eb3c5a94b51f833"
  },
  {
   "metadata": {
    "id": "96a8bb4572d3d6ff"
   },
   "cell_type": "markdown",
   "source": "## Dataset and Imports",
   "id": "96a8bb4572d3d6ff"
  },
  {
   "metadata": {
    "id": "292aec38e7c85950"
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pycaret as pc\n"
   ],
   "id": "292aec38e7c85950",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "a762ae1e506c6e3e"
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('fake_or_real_news.csv', index_col = 0)\n",
    "#df = pd.read_csv('./datasets/fake_or_real_news.csv', index_col = 0) if run in local"
   ],
   "id": "a762ae1e506c6e3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "bbf3f87141d5c4af"
   },
   "cell_type": "markdown",
   "source": "## 0. Exploratory Data Analysis",
   "id": "bbf3f87141d5c4af"
  },
  {
   "metadata": {
    "id": "5cc3a46f2a4cb71",
    "outputId": "0a2ba938-cd99-4d4f-dddd-01b28ecf1dac",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    }
   },
   "cell_type": "code",
   "source": [
    "df.head()"
   ],
   "id": "5cc3a46f2a4cb71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "f2023268c05c1a6b",
    "outputId": "5fa456b2-bb37-426f-9a17-7f4ee0cd7278",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    }
   },
   "cell_type": "code",
   "source": [
    "# set the label class to 1 and 0\n",
    "df['label'] = df['label'].replace({'REAL': 1, 'FAKE': 0})\n",
    "df.head(5)"
   ],
   "id": "f2023268c05c1a6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "aa2de9bd19f63a1d",
    "outputId": "b368118b-82ce-450b-98e4-bcb1ac2bc904",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "df['text'].isnull().sum()"
   ],
   "id": "aa2de9bd19f63a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "4f2fc0dfdeea5e4d",
    "outputId": "6133c223-54cb-48b7-ba0d-747b41f5cee1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "df['text'].isna().sum()"
   ],
   "id": "4f2fc0dfdeea5e4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "652144d383e3e296",
    "outputId": "2988dd6c-9949-4beb-a57e-1a818c5861e9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    }
   },
   "cell_type": "code",
   "source": [
    "# distribution os the labels\n",
    "df['label'].value_counts()"
   ],
   "id": "652144d383e3e296",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "7a5dbc3c53247855"
   },
   "cell_type": "code",
   "source": [
    "df_reduced = df.sample(n=1000, random_state=42)"
   ],
   "id": "7a5dbc3c53247855",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "bf05936be2c22554"
   },
   "cell_type": "markdown",
   "source": [
    "## 1. Natural Language Processing and text vectorization\n",
    "\n",
    "This task will consist of the thematic analysis of the collection provided. The\n",
    "steps you must follow in your work are as follows:\n",
    "\n",
    "• Step 1: Implementation of a pipeline for the preprocessing of the texts.\n",
    "For this task you could use SpaCy, or any other library that you consider\n",
    "appropriate.\n",
    "\n",
    "• Step 2: Text vectorization. In this stage you will analyze the following\n",
    "vectorization schemes:\n",
    "\n",
    "– Classical BoW or TF-IDF representation.\n",
    "\n",
    "– Word2vec/Glove based representation or Doc2Vec vectorization.\n",
    "\n",
    "– Extraction of themes and vector representation of the documents\n",
    "using the LDA algorithm.\n",
    "\n",
    "In the report you must include a description of the preprocessing pipeline\n",
    "used as well as the vectorization strategies that have been explored. For instance, in the Word2vec/FastText based representations you must explain how\n",
    "you convert a set of word vectors into a document vectorization or for the topic\n",
    "model you have to explain how you have carried out the selection of the number\n",
    "of topics. Any additional representation which helps to analyze this vectorization will be welcome (and positively evaluated)."
   ],
   "id": "bf05936be2c22554"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gensim"
   ],
   "metadata": {
    "id": "lnuo1IE8IbGL",
    "outputId": "6eb6411f-43bf-4362-8b58-f68ac608d8ac",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "lnuo1IE8IbGL",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b467c0f4471252de"
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ],
   "id": "b467c0f4471252de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ab4eefe4a94c7331"
   },
   "cell_type": "code",
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "id": "ab4eefe4a94c7331",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return \" \".join(tokens)"
   ],
   "metadata": {
    "id": "_p5Y2srHWZxS"
   },
   "id": "_p5Y2srHWZxS",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_reduced['clean_text'] = (df_reduced['title'] + \" \" + df_reduced['text']).apply(preprocess)"
   ],
   "metadata": {
    "id": "BhhKCnGaWeLS"
   },
   "id": "BhhKCnGaWeLS",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(df_reduced['clean_text'])\n"
   ],
   "metadata": {
    "id": "r0bBfC91W5kl"
   },
   "id": "r0bBfC91W5kl",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "YXEAxXY6XEKf"
   },
   "id": "YXEAxXY6XEKf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import gensim.downloader as api\n",
    "from numpy import mean, zeros\n",
    "\n",
    "# Usar modelo preentrenado (ej: GloVe)\n",
    "w2v = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "def document_vector(doc):\n",
    "    words = doc.split()\n",
    "    vecs = [w2v[word] for word in words if word in w2v]\n",
    "    return mean(vecs, axis=0) if vecs else zeros(100)\n",
    "\n",
    "df_reduced['w2v_vector'] = df_reduced['clean_text'].apply(document_vector)\n"
   ],
   "metadata": {
    "id": "Gi2JgVCsW6Vw"
   },
   "id": "Gi2JgVCsW6Vw",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "texts = [text.split() for text in df_reduced['clean_text']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "lda = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
    "\n",
    "# Vectorización: representación del documento según distribución de tópicos\n",
    "lda_vectors = [lda.get_document_topics(bow) for bow in corpus]\n"
   ],
   "metadata": {
    "id": "YIKv9KqAfCHv"
   },
   "id": "YIKv9KqAfCHv",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(data=df, x='label')\n",
    "plt.title(\"Distribución de etiquetas\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "5Kzn2AL8d8qI",
    "outputId": "36b36079-625f-4fb3-8e2c-3b15cac2d2af",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    }
   },
   "id": "5Kzn2AL8d8qI",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "text_fake = \" \".join(df_reduced[df_reduced.label == 0]['clean_text'])\n",
    "text_real = \" \".join(df_reduced[df_reduced.label == 1]['clean_text'])\n",
    "\n",
    "# FAKE\n",
    "WordCloud(width=800, height=400, background_color='white').generate(text_fake).to_image()\n",
    "# REAL\n",
    "\n"
   ],
   "metadata": {
    "id": "6o3hOH-ad-3a",
    "outputId": "fd5df218-284a-4dbb-fd57-6896b70a8db2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    }
   },
   "id": "6o3hOH-ad-3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "WordCloud(width=800, height=400, background_color='white').generate(text_real).to_image()"
   ],
   "metadata": {
    "id": "HHunxCjHeUQF",
    "outputId": "d5fe8c9b-819a-4610-91d4-42aacc398b8d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    }
   },
   "id": "HHunxCjHeUQF",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pyLDAvis"
   ],
   "metadata": {
    "id": "I3Eh_Q8KemcX",
    "outputId": "0b0b61ad-f6fb-4616-8f0f-23d56ae641eb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "I3Eh_Q8KemcX",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "lda_display = gensimvis.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(lda_display)\n"
   ],
   "metadata": {
    "id": "N2ycmOnhegc-",
    "outputId": "4681456d-7af0-47bf-fda6-cf423d3c528d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    }
   },
   "id": "N2ycmOnhegc-",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Asumiendo que tienes tfidf_matrix o w2v_matrix\n",
    "X_embedded = TSNE(n_components=2).fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=df_reduced['label'])\n",
    "plt.title(\"Visualización de documentos con t-SNE\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "Vf-d-yYoegpg",
    "outputId": "9a5bb484-e5b7-406c-8950-326d2d09fe4c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    }
   },
   "id": "Vf-d-yYoegpg",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_radar(lda_model, corpus, doc_index):\n",
    "    topic_dist = dict(lda_model.get_document_topics(corpus[doc_index], minimum_probability=0))\n",
    "    topics = [f\"Topic {i}\" for i in range(lda_model.num_topics)]\n",
    "    values = [topic_dist.get(i, 0) for i in range(lda_model.num_topics)]\n",
    "\n",
    "    # Radar necesita cerrar el círculo, así que repetimos el primer valor\n",
    "    values += values[:1]\n",
    "    angles = np.linspace(0, 2 * np.pi, len(values), endpoint=True)\n",
    "\n",
    "    # Crear el gráfico\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid', label=f'Doc {doc_index}')\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "    # Configurar ejes\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(topics, fontsize=10)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(f\"Distribución de Tópicos - Documento {doc_index}\", size=14, pad=20)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ],
   "metadata": {
    "id": "6GBU8ZlMegtC"
   },
   "id": "6GBU8ZlMegtC",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plot_radar(lda, corpus, doc_index=3)  # Elige el índice del documento que quieras visualizar\n"
   ],
   "metadata": {
    "id": "j7ZFmC2fgCF6",
    "outputId": "4ef12c25-22f0-4007-d978-147241319230",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    }
   },
   "id": "j7ZFmC2fgCF6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Ver las 10 palabras más importantes de cada tópico\n",
    "for topic_id, topic_words in lda.print_topics(num_topics=lda.num_topics, num_words=10):\n",
    "    print(f\"Topic {topic_id}: {topic_words}\")\n"
   ],
   "metadata": {
    "id": "f86aNiPugVSe",
    "outputId": "3967ffa9-0000-43b0-c657-9200ab0cf4d4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "f86aNiPugVSe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "sns.heatmap(similarity_matrix[:20,:20], cmap=\"coolwarm\")\n",
    "plt.title(\"Similitud de documentos (TF-IDF)\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "h-m2OrWiegxe",
    "outputId": "e74c7167-35fa-42d2-8d0b-3f547b838c7d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    }
   },
   "id": "h-m2OrWiegxe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "1bbea3b856eefa9"
   },
   "cell_type": "markdown",
   "source": "## 2. Machine Learning",
   "id": "1bbea3b856eefa9"
  },
  {
   "metadata": {
    "id": "755cc2a8d739060a"
   },
   "cell_type": "markdown",
   "source": [],
   "id": "755cc2a8d739060a"
  },
  {
   "metadata": {
    "id": "b94fdeed268db656"
   },
   "cell_type": "markdown",
   "source": "### 2.1. Classification, Regression using feature extraction or selection techniques",
   "id": "b94fdeed268db656"
  },
  {
   "metadata": {
    "id": "504a66e89f2cbcaa"
   },
   "cell_type": "markdown",
   "source": [
    "Implementation and evaluation of the performance of a classifier or regression\n",
    "model for the selected dataset. Use one of the metadata available in the dataset\n",
    "as your target variable: a categorical variable if you opt for a classification task,\n",
    "or a real type variable for regression. Note that discrete but ordered variables\n",
    "(such as dates, scores, etc.) can also be used as target variables for a regression\n",
    "task.\n",
    "\n",
    "For this task, you will need to compare the performance by using the different\n",
    "document vectorizations. In addition, you must use for your work some of the\n",
    "feature extraction or selection algorithms described in the course, analyzing\n",
    "their impact on the results obtained. Use the usual metrics for performance\n",
    "analysis, i.e., error rates, ROC curves, confusion matrices, etc., if you pose\n",
    "a classification task, or the root mean square error if you choose a regression\n",
    "model.\n",
    "\n",
    "To adjust the hyperparameters of the classification or regression models, you\n",
    "must use a validation methodology that must also be explained in the report."
   ],
   "id": "504a66e89f2cbcaa"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE"
   ],
   "metadata": {
    "id": "kczNctu5mSsw"
   },
   "id": "kczNctu5mSsw",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Vectorización TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=3000)\n",
    "X_tfidf = tfidf.fit_transform(df_reduced[\"clean_text\"])\n",
    "y = df_reduced[\"label\"]\n",
    "# Selección de características\n",
    "selector = SelectKBest(chi2, k=1000)\n",
    "X_selected = selector.fit_transform(X_tfidf, y)\n",
    "\n",
    "# Entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"SVM\": SVC(probability=True)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    print(f\"\\n\\n==== {name} ====\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix - {name}\")\n",
    "    plt.show()\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc_score(y_test, y_proba):.2f})\")\n",
    "\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "id": "zPu6VhcjmFx3",
    "outputId": "cd0b31c3-c311-47f9-b0f5-3aea37c54dde",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "id": "zPu6VhcjmFx3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b35266881a67d5db"
   },
   "cell_type": "markdown",
   "source": "### 2.2. Clustering using feature extraction or selection techniques",
   "id": "b35266881a67d5db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In case the dataset does not have a clear variable that can be used for document\n",
    "classification or to solve a regression problem, this task can be approached as\n",
    "an unsupervised learning task and document clustering can be performed.\n",
    "\n",
    "In this case the clustering results obtained using the different vectorizations\n",
    "obtained from the previous task should be explored. To analyze and compare\n",
    "the results with each other, measures based on clustering consistency such as\n",
    "the silhouette coefficient1\n",
    "can be used or an analysis of the obtained clusters\n",
    "(centroids and distribution of documents in each cluster) can be carried out.\n",
    "\n",
    "For the selection of the optimal number of centroids, the analysis of the\n",
    "silhouette coefficient or other measures specific to the particular algorithm (e.g.\n",
    "the sum of squared distances of samples to their closest cluster center for the\n",
    "K-means) can also be used.\n",
    "In addition, you must include here some of the feature extraction or selection\n",
    "algorithms described in the course, analyzing their impact on the clustering\n",
    "results.\n"
   ],
   "metadata": {
    "id": "g0UrWzsOJPVp"
   },
   "id": "g0UrWzsOJPVp"
  },
  {
   "cell_type": "code",
   "source": [
    "# === TASK 2.2: Clustering de Noticias ===\n",
    "\n",
    "# Vectorización para clustering\n",
    "X_cluster = TfidfVectorizer(max_features=1000).fit_transform(df_reduced[\"clean_text\"])\n",
    "\n",
    "# Método del codo\n",
    "sse = []\n",
    "for k in range(2, 80):\n",
    "    km = KMeans(n_clusters=k, random_state=42).fit(X_cluster)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "plt.plot(range(2,80), sse, marker='o')\n",
    "plt.xlabel(\"Número de clusters\")\n",
    "plt.ylabel(\"SSE (Inercia)\")\n",
    "plt.title(\"Método del Codo\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Elegimos k=4 como ejemplo\n",
    "k = 10\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "df_reduced['cluster'] = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "# Visualización t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=40, random_state=42)\n",
    "tsne_results = tsne.fit_transform(X_cluster.toarray())\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for c in range(k):\n",
    "    plt.scatter(tsne_results[df_reduced.cluster == c, 0], tsne_results[df_reduced.cluster == c, 1], label=f\"Cluster {c}\")\n",
    "plt.legend()\n",
    "plt.title(\"Clusters de Noticias (t-SNE)\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Evaluación de clustering\n",
    "score = silhouette_score(X_cluster, df_reduced[\"cluster\"])\n",
    "print(f\"Silhouette Score: {score:.3f}\")"
   ],
   "metadata": {
    "id": "5FpKmTfImK0Y",
    "outputId": "233480a8-c8c1-45ef-e2b5-963a98025353",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "id": "5FpKmTfImK0Y",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "c61f49a249415514"
   },
   "cell_type": "markdown",
   "source": "### 2.3. Recommendation Systems",
   "id": "c61f49a249415514"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Función para predecir si una noticia es FAKE o REAL\n",
    "def predecir_noticia(texto):\n",
    "    texto_preprocesado = preprocess(texto)  # Preprocesamiento\n",
    "    vectorizado = tfidf.transform([texto_preprocesado])  # Vectorización\n",
    "    prediccion = models[\"Logistic Regression\"].predict(vectorizado)\n",
    "    return \"FAKE\" if prediccion == 0 else \"REAL\"\n",
    "\n",
    "# Función para recomendar noticias contrarias\n",
    "def recomendar_similares(texto, top_n=3):\n",
    "    # Predecir si la noticia es FAKE o REAL\n",
    "    prediccion = predecir_noticia(texto)\n",
    "    print(f\"Predicción: {prediccion}\")\n",
    "\n",
    "    # Vectorizar la noticia\n",
    "    texto_preprocesado = preprocess(texto)\n",
    "    vectorizado = tfidf.transform([texto_preprocesado])\n",
    "\n",
    "    # Calcular similitudes entre la noticia y las demás\n",
    "    similitudes = cosine_similarity(vectorizado, X_tfidf).flatten()\n",
    "\n",
    "    # Buscar noticias contrarias (si la noticia es FAKE, buscamos REAL)\n",
    "    if prediccion == \"FAKE\":\n",
    "        recomendaciones = df[y == 1]  # Selecciona las noticias REAL\n",
    "    else:\n",
    "        recomendaciones = df[y == 0]  # Selecciona las noticias FAKE\n",
    "\n",
    "    # Ordenar por similitud\n",
    "    idx_recomendados = np.argsort(similitudes)[::-1][1:top_n+1]  # Ignorar la misma noticia\n",
    "    recomendados = recomendaciones.iloc[idx_recomendados]\n",
    "\n",
    "    return recomendados[['title', 'label']], similitudes[idx_recomendados]\n",
    "\n",
    "# Ejemplo de recomendación\n",
    "texto_noticia = \"Your input text here to predict and recommend news\"\n",
    "recomendados, similitudes = recomendar_similares(texto_noticia)\n",
    "print(\"Recomendaciones:\")\n",
    "for i, (title, label) in enumerate(recomendados.itertuples(index=False), 1):\n",
    "    print(f\"{i}. {title} - {label} (Similitud: {similitudes[i-1]:.2f})\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Función para predecir si una noticia es FAKE o REAL\n",
    "def predecir_noticia(texto):\n",
    "    texto_preprocesado = preprocess(texto)  # Preprocesamiento\n",
    "    vectorizado = tfidf.transform([texto_preprocesado])  # Vectorización\n",
    "    prediccion = models[\"Logistic Regression\"].predict(vectorizado)\n",
    "    return \"FAKE\" if prediccion == 0 else \"REAL\"\n",
    "\n",
    "# Función para recomendar noticias contrarias\n",
    "def recomendar_similares(texto, top_n=3):\n",
    "    # Predecir si la noticia es FAKE o REAL\n",
    "    prediccion = predecir_noticia(texto)\n",
    "    print(f\"Predicción: {prediccion}\")\n",
    "\n",
    "    # Vectorizar la noticia\n",
    "    texto_preprocesado = preprocess(texto)\n",
    "    vectorizado = tfidf.transform([texto_preprocesado])\n",
    "\n",
    "    # Calcular similitudes entre la noticia y las demás\n",
    "    similitudes = cosine_similarity(vectorizado, X_tfidf).flatten()\n",
    "\n",
    "    # Buscar noticias contrarias (si la noticia es FAKE, buscamos REAL)\n",
    "    if prediccion == \"FAKE\":\n",
    "        recomendaciones = df[y == 1]  # Selecciona las noticias REAL\n",
    "    else:\n",
    "        recomendaciones = df[y == 0]  # Selecciona las noticias FAKE\n",
    "\n",
    "    # Ordenar por similitud\n",
    "    idx_recomendados = np.argsort(similitudes)[::-1][1:top_n+1]  # Ignorar la misma noticia\n",
    "    recomendados = recomendaciones.iloc[idx_recomendados]\n",
    "\n",
    "    return recomendados[['title', 'label']], similitudes[idx_recomendados]\n",
    "\n",
    "# Ejemplo de recomendación\n",
    "texto_noticia = \"Your input text here to predict and recommend news\"\n",
    "recomendados, similitudes = recomendar_similares(texto_noticia)\n",
    "print(\"Recomendaciones:\")\n",
    "for i, (title, label) in enumerate(recomendados.itertuples(index=False), 1):\n",
    "    print(f\"{i}. {title} - {label} (Similitud: {similitudes[i-1]:.2f})\")\n"
   ],
   "metadata": {
    "id": "3XmJxkmhoul8",
    "outputId": "836635ea-af45-4978-e22a-680f994c0755",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    }
   },
   "id": "3XmJxkmhoul8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Similitud de coseno entre todas las noticias\n",
    "similarity_matrix = cosine_similarity(X_selected)\n",
    "\n",
    "# Función para recomendar noticias similares de diferente clase\n",
    "def recomendar_contrarias(indice_noticia, top_n=3):\n",
    "    etiqueta = y.iloc[indice_noticia]\n",
    "    similitudes = similarity_matrix[indice_noticia]\n",
    "    similares_idx = np.argsort(similitudes)[::-1]  # índices más similares primero\n",
    "    recomendaciones = []\n",
    "    for idx in similares_idx[1:]:  # omitir la misma noticia\n",
    "        if y.iloc[idx] != etiqueta:\n",
    "            recomendaciones.append((idx, similitudes[idx]))\n",
    "        if len(recomendaciones) == top_n:\n",
    "            break\n",
    "    return recomendaciones\n",
    "\n",
    "# Ejemplo: recomendar para una noticia REAL\n",
    "idx = y[y == 1].index[0]\n",
    "print(f\"\\nNoticia de referencia:\\n{df_reduced.loc[idx, 'title']}\")\n",
    "print(\"Recomendaciones de noticias FAKE similares:\")\n",
    "\n",
    "print(y)\n",
    "for rec_idx, sim in recomendar_contrarias(idx):\n",
    "    print(f\"- [{sim:.2f}] {df_reduced.loc[rec_idx, 'title']}\")\n"
   ],
   "metadata": {
    "id": "1y7i_o2anuqA",
    "outputId": "f1ccbea5-87cd-4465-ccd8-4874156e4418",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    }
   },
   "id": "1y7i_o2anuqA",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Análisis Temático - Noticias FAKE vs REAL\"),\n",
    "\n",
    "    dcc.Graph(id='histogram-topic',\n",
    "              figure=px.histogram(df, x='topic', color='label', barmode='group', title='Distribución de temas por etiqueta')),\n",
    "\n",
    "    dcc.Graph(id='box-confidence'),\n",
    "    dcc.Graph(id='scatter-topics'),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('box-confidence', 'figure'),\n",
    "    Output('scatter-topics', 'figure'),\n",
    "    Input('histogram-topic', 'clickData')\n",
    ")\n",
    "def update_graphs(clickData):\n",
    "    if clickData is None:\n",
    "        dff = df.copy()\n",
    "        title_suffix = \" (todos los temas)\"\n",
    "    else:\n",
    "        selected_topic = clickData['points'][0]['x']\n",
    "        dff = df[df['topic'] == int(selected_topic)]\n",
    "        title_suffix = f\" (tema {selected_topic})\"\n",
    "\n",
    "    fig_box = px.box(dff, x='label',\n",
    "                     y=doc_topics[dff.index].max(axis=1),\n",
    "                     title=f'Confianza máxima en tema por etiqueta {title_suffix}')\n",
    "\n",
    "    fig_scatter = px.scatter(dff, x=doc_topics[dff.index, 0], y=doc_topics[dff.index, 1],\n",
    "                             color='label', title=f'Mapa 2D temas 0 vs 1 {title_suffix}')\n",
    "\n",
    "    return fig_box, fig_scatter\n"
   ],
   "metadata": {
    "id": "CHf9kVyPoe7-",
    "outputId": "0d755b86-d4bf-478c-feb2-159f47b998ee",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    }
   },
   "id": "CHf9kVyPoe7-",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "gausLKWwJe9y"
   },
   "id": "gausLKWwJe9y"
  },
  {
   "metadata": {
    "id": "b2b444bcf1da2dad"
   },
   "cell_type": "markdown",
   "source": " ## 3. Implementation of a dashboard using the Python Dash library",
   "id": "b2b444bcf1da2dad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Students need to implement a dashboard using Python library Dash. Said\n",
    "dashboard must include at least one figure related to the predominant LDA\n",
    "topic of each document, as well as a minimum of two additional representations\n",
    "related to other variables. Your dashboard must be interactive, i.e., the user\n",
    "will be able to make selections in one or more of the included charts, and the\n",
    "rest of the charts will modify their values according to the selection made."
   ],
   "metadata": {
    "id": "Be65qQLxJcEf"
   },
   "id": "Be65qQLxJcEf"
  },
  {
   "metadata": {
    "id": "b578cbeb2cfa03ef"
   },
   "cell_type": "markdown",
   "source": "## 4. Final report and presentation.",
   "id": "b578cbeb2cfa03ef"
  },
  {
   "metadata": {
    "id": "f366451e666d847f"
   },
   "cell_type": "markdown",
   "source": [],
   "id": "f366451e666d847f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
